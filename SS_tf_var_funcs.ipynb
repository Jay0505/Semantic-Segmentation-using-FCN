{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "VGG_16_weights_dict = np.load('/Users/vijay/Downloads/vgg16_weights.npz')\n",
    "\n",
    "\n",
    "############################################\n",
    "\n",
    "def get_placeholders_for_train_label_data(no_of_classes):\n",
    "    \n",
    "    x       = tf.placeholder(tf.float32, shape = [None, 224 * 224 * 3], name = 'x_rs')\n",
    "    x_rs    = tf.reshape(x, shape = [-1, 224, 224, 3])\n",
    "    \n",
    "    y       = tf.placeholder(tf.float32, shape = [None, no_of_classes])\n",
    "    y_rs    = tf.reshape(y, shape = [-1, no_of_classes])\n",
    "    dropout = tf.placeholder(tf.float32, name = 'dropout')\n",
    "    \n",
    "    return x_rs, y_rs, dropout\n",
    "\n",
    "\n",
    "\n",
    "############################################\n",
    "\n",
    "def get_weights_or_bias_from_VGG(wts_or_bias_name):\n",
    "    return VGG_16_weights_dict[wts_or_bias_name]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############################################\n",
    "\n",
    "def get_weights_or_bias_as_variables(wts_or_bias_name, var_name):\n",
    "    wts_or_bias    = get_weights_or_bias_from_VGG(wts_or_bias_name)\n",
    "    wt_or_bias_init = tf.constant_initializer(values = wts_or_bias, dtype = tf.float32)\n",
    "    wt_or_bias_var  = tf.get_variable(shape = np.shape(wts_or_bias, initializer = wt_or_bias_init, name = var_name))\n",
    "    \n",
    "    return wt_or_bias_var\n",
    "\n",
    "\n",
    "\n",
    "############################################\n",
    "\n",
    "def fetch_and_reshape_fc_weights(fc_wts_name, rs_shape):\n",
    "    fc_wts      = VGG_16_weights_dict[fc_wts_name]\n",
    "    fc_wts      = fc_wts.reshape(rs_shape)\n",
    "    fc_wts_init = tf.constant_initializer(values = fc_wts, dtype = tf.float32)\n",
    "    fc_wts_var  = tf.get_variable(shape = rs_shape, initializer = fc_wts_init, name = 'fc_wts')\n",
    "    \n",
    "    return fc_wts_var\n",
    "\n",
    "\n",
    "\n",
    "############################################\n",
    "\n",
    "def conv_layer(data, flt_name, bias_name, res_name):\n",
    "\n",
    "    with tf.variable_scope(res_name):\n",
    "        conv_wt       = get_weights_or_bias_as_variables(flt_name, 'conv_wt')\n",
    "        conv_bias     = get_weights_or_bias_as_variables(bias_name, 'conv_bias')\n",
    "        \n",
    "        conv_res      = tf.nn.conv2d(data, filter = conv_wt, strides = [1, 1, 1, 1], padding = 'SAME')\n",
    "        conv_res_bias = tf.nn.bias_add(conv_res, conv_bias)\n",
    "        conv_res_relu = tf.nn.relu(conv_res_bias)\n",
    "\n",
    "    return conv_res_relu\n",
    "\n",
    "\n",
    "\n",
    "############################################\n",
    "\n",
    "def pooling_layer(data, pool_name):\n",
    "    with tf.variable_scope(pool_name):\n",
    "        return tf.nn.max_pool(value = data, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')\n",
    "\n",
    "    \n",
    "\n",
    "############################################\n",
    "\n",
    "def fc_layer(data, dropout, fc_wts_name = None, fc_bias_name = None):\n",
    "    \n",
    "    if fc_wts_name is not None and fc_bias_name is not None:\n",
    "        fc_wt_shape = []\n",
    "        \n",
    "        if fc_wts_name == 'fc6_W':\n",
    "            fc_wt_shape = [7, 7, 512, 4096]\n",
    "        if fc_wts_name == 'fc7_W':\n",
    "            fc_wt_shape = [1, 1, 4096, 4096]\n",
    "        \n",
    "        with tf.variable_scope(fc_wts_name):\n",
    "            fc_wts = fetch_and_reshape_fc_weights(fc_wts_name, fc_wt_shape)            \n",
    "            fc_bias = get_weights_or_bias_as_variables(fc_bias_name, 'fc_b')\n",
    "            \n",
    "            fc_conv = tf.nn.conv2d(data, filter = fc_wts, strides = [1, 1, 1, 1], padding = 'SAME')\n",
    "            fc_conv_bias = tf.nn.bias_add(fc_conv, fc_bias)\n",
    "            fc_conv_relu = tf.nn.relu(fc_conv_bias)\n",
    "            \n",
    "        return tf.nn.droput(fc_conv_relu, dropout)\n",
    "            \n",
    "\n",
    "        \n",
    "############################################\n",
    "\n",
    "def skip_connection(from_data, con_name, flt_val):\n",
    "    with tf.variable_scope(con_name):\n",
    "        skp_wt = tf.Variable(tf.truncated_normal(flt_val, stddev = 0.2, seed = 23, dtype = tf.float32), name = 'skip_wt')\n",
    "        skp_conv = tf.nn.conv2d(from_data, skp_wt, [1, 1, 1, 1], padding = 'SAME')\n",
    "        \n",
    "        skp_bias = tf.Variable(tf.constant(0.1, shape = [flt_val[-1]], dtype = tf.float32), name = 'skip_bias')\n",
    "        skip_res = tf.bias_add(skp_conv, skp_bias)\n",
    "        return skip_res\n",
    "\n",
    "    \n",
    "\n",
    "############################################\n",
    "\n",
    "def upsample(from_data, op_shape, no_of_channels, name, factor):\n",
    "    flt_size = 2 * factor - factor % 2\n",
    "    strides = [1, factor, factor, 1]\n",
    "    with tf.variable_scope(name):\n",
    "        filter_shape = [flt_size, flt_size, no_of_channels, no_of_channels]\n",
    "        weights = get_bilinear_weights(filter_shape, factor)\n",
    "        deconv = tf.nn.conv2d_transpose(from_data, weights, op_shape, strides = strides, padding = 'SAME')\n",
    "        upsmp_bias = tf.Variable(tf.constant(0.1, shape = [op_shape[-]]), name = 'upsmp_bias')\n",
    "        \n",
    "        deconv_bias = tf.bias_add(deconv, upsmp_bias)\n",
    "        \n",
    "    return deconv_bias\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############################################\n",
    "\n",
    "def get_bilinear_weights(filter_shape, factor):\n",
    "    flt_size = filter_shape[0]\n",
    "    if flt_size % 2 == 0:\n",
    "        centre = factor - 1\n",
    "    else:\n",
    "        centre = factor - 0.5\n",
    "        \n",
    "    bilinear = np.zeros([flt_size, flt_size])\n",
    "    weigths = np.zeros(filter_shape)\n",
    "    for row in range(filter_shape[0]):\n",
    "        for column in range(filter_shape[1]):\n",
    "            cell_val = (1 - abs((row - centre) / factor)) * (1 - abs((column - centre) / factor))\n",
    "            bilinear[row, column] = cell_val\n",
    "    \n",
    "    for val in range(filter_shape[2]):\n",
    "        weights[:, :, val, val] = bilinear\n",
    "    \n",
    "    wt_init = tf.constant_initializer(value = weights, dtype = tf.float32)\n",
    "    bilinear_weights = tf.Variable(initializer = wt_init, name = 'blnr_wt', shape = filter_shape)\n",
    "    \n",
    "    return bilinear_weights\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 512, 512)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(VGG_16_weights_dict['conv5_3_W'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
